{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform tif images to JPG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def convert_all_tif_to_jpg(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"folder {folder_path} not exists\")\n",
    "        return\n",
    "    output_folder = os.path.join(folder_path, \"test_jpg\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".tif\") or filename.endswith(\".tiff\"):\n",
    "            tif_path = os.path.join(folder_path, filename)\n",
    "            tif_image = Image.open(tif_path).convert('L')\n",
    "\n",
    "            rgb_image = tif_image.convert('RGB')\n",
    "    \n",
    "            jpg_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "            jpg_path = os.path.join(output_folder, jpg_filename)\n",
    "            rgb_image.save(jpg_path, 'JPEG')\n",
    "            print(f\"changed {filename} to {jpg_filename}\")\n",
    "        else:\n",
    "            print(f\"skipped {filename} because it is not a tif file\")\n",
    "    \n",
    "    print(\"all done!\")\n",
    "\n",
    "folder_path = 'DIC_crack_dataset/test'\n",
    "convert_all_tif_to_jpg(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = read_image(\"SVS_2_LS18_to_LS19_RS2_0000_1_173_2816_2304.jpg\")\n",
    "# show the img\n",
    "plt.imshow(img.permute(1, 2, 0)); plt.axis('off'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet18\n",
    "from torchcam.methods import SmoothGradCAMpp,GradCAM , GradCAMpp, ScoreCAM,LayerCAM, SSCAM,GradCAMpp, ISCAM, XGradCAM\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "for img_path in os.listdir('test_img'):\n",
    "    img = read_image(os.path.join('test_img', img_path))\n",
    "    # Preprocess it for your chosen model\n",
    "    input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    model.load_state_dict(torch.load('checkpoints/output/epoch=99-val_loss=0.00.ckpt'))\n",
    "    model.eval()  # Set the model to evaluation mode after loading\n",
    "\n",
    "    cam_extractor = ScoreCAM(model, 'layer4')\n",
    "    out=(model(input_tensor.unsqueeze(0)))\n",
    "    activation_map=(cam_extractor(out.squeeze(0).argmax().item(), out))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torchcam.utils import overlay_mask\n",
    "\n",
    "    # Resize the CAM and overlay it\n",
    "    result = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.999)\n",
    "    # Display it\n",
    "    plt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# load the checkpoint\n",
    "ckpt = torch.load('checkpoints/output/epoch=99-val_loss=0.00.ckpt')\n",
    "# remove \"model.\" from the state_dict keys\n",
    "ckpt = ckpt['state_dict']\n",
    "ckpt = {k.replace(\"model.\", \"\"): v for k, v in ckpt.items()} # remove \"model.\" from the state_dict keys\n",
    "# save \n",
    "torch.save(ckpt, 'checkpoints/output/epoch=99-val_loss=0.00.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet18\n",
    "from torchcam.methods import CAM, GradCAM, ScoreCAM,LayerCAM, SSCAM,GradCAMpp, ISCAM, XGradCAM, SmoothGradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 0: seed\n",
    "def set_seed(seed):\n",
    "    # Python built-in random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch random seed (for CPU and GPU)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # If you are using GPU and want deterministic behavior\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "    # Ensuring deterministic behavior in PyTorch (may slow down performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set seed\n",
    "set_seed(3407)\n",
    "\n",
    "# Step 1: read image\n",
    "img = read_image(\"SVS_1_SC4_LS28_to_LS29_0000_0_39_768_1024.jpg\")\n",
    "input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Step 2: load model\n",
    "model = resnet18(pretrained=True)\n",
    "# model.fc = nn.Linear(512, 2)\n",
    "# model.load_state_dict(torch.load('checkpoints/output/epoch=99-val_loss=0.00.ckpt'))\n",
    "model.eval()  # Set the model to evaluation mode after loading\n",
    "cam_extractor = GradCAM(model, 'layer4')\n",
    "out = model(input_tensor.unsqueeze(0))\n",
    "\n",
    "# Step 3: extract activation map\n",
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "# Step 4:  convert to numpy array\n",
    "heatmap = activation_map[0].squeeze(0).detach().numpy()\n",
    "\n",
    "# smooth the heatmap values to avoid too dispersed sampling probabilities, enhance the sampling probability of the high temperature area\n",
    "heatmap = np.clip(heatmap, 0, None)  # remove negative values\n",
    "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())  # normalize to [0, 1]\n",
    "\n",
    "# Step 5: resize heatmap to match the original image size\n",
    "heatmap_resized = cv.resize(heatmap, (img.shape[2], img.shape[1]))\n",
    "\n",
    "# Step 6: sampling logic\n",
    "def sample_points(heatmap, num_points=10, weight_exponent=1):\n",
    "    height, width = heatmap.shape\n",
    "    flat_heatmap = heatmap.flatten()\n",
    "\n",
    "    flat_heatmap = np.power(flat_heatmap, weight_exponent)\n",
    "\n",
    "    probabilities = flat_heatmap / flat_heatmap.sum() \n",
    "    indices = np.arange(len(flat_heatmap))\n",
    "\n",
    "    sampled_indices = np.random.choice(indices, size=num_points, replace=False, p=probabilities)\n",
    "\n",
    "    sampled_coords = [(index % width, index // width) for index in sampled_indices]\n",
    "\n",
    "    return sampled_coords\n",
    "\n",
    "# Step 7: get sampled points\n",
    "sampled_points = sample_points(heatmap_resized, num_points=10, weight_exponent=1)\n",
    "\n",
    "# Step 8:  load SAM model\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Step 9: convert image to PIL format\n",
    "image = to_pil_image(img)\n",
    "predictor.set_image(np.array(image))\n",
    "\n",
    "# Step 11: convert sampled points to the format required by the SAM model\n",
    "input_points = np.array(sampled_points)\n",
    "input_labels = np.ones(input_points.shape[0])\n",
    "\n",
    "# Step 10: run SAM model\n",
    "masks, scores, _ = predictor.predict(\n",
    "    point_coords=input_points, \n",
    "    point_labels=input_labels,\n",
    "    multimask_output=False\n",
    ")\n",
    "\n",
    "\n",
    "# Step 12:  determine whether the foreground or background is larger\n",
    "foreground_count = np.sum(masks)\n",
    "background_count = np.size(masks) - foreground_count\n",
    "if foreground_count > background_count:\n",
    "    masks = np.logical_not(masks)\n",
    "\n",
    "# Step 12: post-processing of the mask\n",
    "def post_process_mask(mask):\n",
    "    # Convert mask to uint8 for OpenCV operations\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "\n",
    "    kernel = np.ones((3, 3), np.uint8)  \n",
    "    mask = cv.morphologyEx(mask, cv.MORPH_OPEN, kernel)\n",
    "    mask = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel)\n",
    "\n",
    "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    min_area = 100\n",
    "    max_area = 15000\n",
    "\n",
    "    filtered_image = np.zeros_like(mask)\n",
    "\n",
    "    for contour in contours:\n",
    "        area = cv.contourArea(contour)\n",
    "        if min_area < area < max_area:\n",
    "            cv.drawContours(filtered_image, [contour], -1, 255, thickness=cv.FILLED)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "processed_mask = post_process_mask(masks[0])\n",
    "\n",
    "    \n",
    "# Step 12: visualize the results\n",
    "def show_points(coords, labels, ax, marker_size=150):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    \n",
    "plt.figure(figsize=(20, 50))\n",
    "\n",
    "# original\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Original')\n",
    "\n",
    "# CAM overlay\n",
    "result = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(result)\n",
    "plt.title('CAM Overlay')\n",
    "plt.axis('off')\n",
    "\n",
    "# show sampled points\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(image)\n",
    "show_points(input_points, input_labels, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.title('Pointed')    \n",
    "\n",
    "\n",
    "# show SAM mask\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(image)\n",
    "plt.imshow(masks[0], cmap='jet', alpha=0.5)  # 使用半透明的掩码叠加到原图上\n",
    "plt.axis('off')\n",
    "plt.title('SAM')\n",
    "\n",
    "# show post-processed mask\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(image)\n",
    "plt.imshow(processed_mask, cmap='jet', alpha=0.5)  # 叠加处理后的掩码\n",
    "plt.axis('off')\n",
    "plt.title('Post-Processed')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
